{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC4-4"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent",
        "format_version": "1.3",
        "jupytext_version": "1.5.2"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Chatbots.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickqiu/trax_chatbots/blob/main/chatbots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxKFPeuh_XEm"
      },
      "source": [
        "# Chatbots with Pretrained Reformer Model\n",
        "\n",
        "The notebook demonstrates conversations between two chatbots in single or multiple domains of attraction, hospital, hotel, police, taxi and train using a pretrained Reformer model.\n",
        "\n",
        "Credits should be given to Lukasz Kaiser and his Trax open source team. Here is a list of useful resources.\n",
        "\n",
        "- [MultiWoz](https://arxiv.org/abs/1810.00278) dataset\n",
        "- [Reformer](https://arxiv.org/abs/2001.04451) paper\n",
        "- [Trax](https://github.com/google/trax) github repository\n",
        "- [Natural Language Processing Specialization](https://www.deeplearning.ai/natural-language-processing-specialization/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_S5N0jFY0VQ",
        "outputId": "a29fdda8-866b-45e3-a253-2c3eafba914f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "!pip install -U https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl\n",
        "!pip install -q -U jax\n",
        "!pip install -q -U trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jaxlib==0.1.55\n",
            "\u001b[?25l  Downloading https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl (144.8MB)\n",
            "\u001b[K     |████████████████████████████████| 144.8MB 21kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.55) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.55) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.55) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jaxlib==0.1.55) (1.15.0)\n",
            "Installing collected packages: jaxlib\n",
            "  Found existing installation: jaxlib 0.1.55\n",
            "    Uninstalling jaxlib-0.1.55:\n",
            "      Successfully uninstalled jaxlib-0.1.55\n",
            "Successfully installed jaxlib-0.1.55\n",
            "\u001b[K     |████████████████████████████████| 481kB 2.7MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 419kB 2.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 12.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 22.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 55.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 36.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 57.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 54.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 57.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 52.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 45.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 655kB 66.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6MB 58.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 74.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 62.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 76.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 72.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 76.0MB/s \n",
            "\u001b[?25h  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV4zpTnSVFIp",
        "outputId": "da49f802-ac4d-4f93-d21b-04d315f9d14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import packages\n",
        "import numpy as np\n",
        "import trax   \n",
        "from trax import layers as tl\n",
        "!pip list | grep trax"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.5                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0kcMWhE4Qw3",
        "outputId": "7f322d03-8f75-4482-e1cd-4192bb00fbaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# clone for vocabs and the pretrained model parts from repository\n",
        "!git clone https://github.com/rickqiu/trax_chatbots.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'trax_chatbots'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (23/23), done.\u001b[K\n",
            "remote: Total 40 (delta 12), reused 0 (delta 0), pack-reused 17\u001b[K\n",
            "Unpacking objects: 100% (40/40), done.\n",
            "Checking out files: 100% (11/11), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRO41Ip4467",
        "outputId": "e19284fc-ce1e-4b7d-e70a-52d67e5bd0e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "%cd trax_chatbots\n",
        "!tar -xzvf vocabs.tar.gz\n",
        "!cat model_splits/* > chatbot_model1.pkl.gz\n",
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/trax_chatbots\n",
            "vocabs/\n",
            "vocabs/en_32k.sentencepiece\n",
            "vocabs/en_32k.sentencepiece.vocab\n",
            "vocabs/en_32k.subword\n",
            "vocabs/en_8k.subword\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkd8zeumYviy"
      },
      "source": [
        "# define attention for fast inference\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "# define the model\n",
        "model = trax.models.reformer.ReformerLM( \n",
        "        # set vocab size\n",
        "        vocab_size=33000,\n",
        "        # set number of layers\n",
        "        n_layers=6,\n",
        "        # set mode\n",
        "        mode='predict',\n",
        "        # set attention type\n",
        "        attention_type=attention\n",
        "    )\n",
        "\n",
        "# define an input signature\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "\n",
        "# initialize from file\n",
        "model.init_from_file('trax_chatbots/chatbot_model1.pkl.gz',\n",
        "                     weights_only=True, input_signature=shape11)\n",
        "\n",
        "# save the starting state\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ7EF6exYvhG"
      },
      "source": [
        "# Reference: Course 4 of https://www.deeplearning.ai/natural-language-processing-specialization/\n",
        "# vocabulary file directory\n",
        "VOCAB_DIR = './trax_chatbots/vocabs'\n",
        "\n",
        "# vocabulary filename\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "\n",
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "\n",
        "\n",
        "def generate_output(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you just trained\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create input tokens using the the tokenize function\n",
        "    input_tokens =  tokenize(start_sentence, vocab_file, vocab_dir)\n",
        "    \n",
        "    # Add batch dimension to array. Convert from (n,) to (x, n) where \n",
        "    # x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)\n",
        "    input_tokens_with_batch = np.expand_dims(input_tokens, axis=0)\n",
        "    \n",
        "    # call the autoregressive_sample_stream function from trax\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
        "        # model\n",
        "        ReformerLM,\n",
        "        # inputs will be the tokens with batch dimension\n",
        "        inputs=input_tokens_with_batch,\n",
        "        # temperature\n",
        "        temperature=temperature\n",
        "    )\n",
        "    \n",
        "    return output_gen\n",
        "\n",
        "\n",
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you just trained\n",
        "        model_state (np.array): initial state of the model before decoding\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        max_len (int): maximum number of tokens to generate \n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"  \n",
        "    \n",
        "    # define the delimiters we used during training\n",
        "    delimiter_1 = 'Person 1: ' \n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    # initialize detokenized output\n",
        "    sentence = ''\n",
        "    \n",
        "    # token counter\n",
        "    counter = 0\n",
        "    \n",
        "    # output tokens. we insert a ': ' for formatting\n",
        "    #result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "    result = []\n",
        "    \n",
        "    # reset the model state when starting a new dialogue\n",
        "    ReformerLM.state = model_state\n",
        "    \n",
        "    # calls the output generator implemented earlier\n",
        "    output = generate_output(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    \n",
        "    # print the starting sentence\n",
        "    print(start_sentence.split(delimiter_2)[0].strip())\n",
        "    \n",
        "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\n",
        "    for o in output:\n",
        "        \n",
        "        result.append(o)\n",
        "        \n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        \n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "        \n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        counter += 1\n",
        "        \n",
        "        if counter > max_len:\n",
        "            break    "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flhDzydFYvjK",
        "outputId": "131f072d-79f4-42ad-eb34-00755c8643a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Are there theatres in town?\n",
            "Person 2: There are 4 theatres in town. Do you have a preference on area? \n",
            "Person 1: No, but I would like the one in the south. \n",
            "Person 2: I would recommend the Junction. Would you like more information on it? \n",
            "Person 1: Yes, I would like the address and postcode please. \n",
            "Person 2: The address is Clifton Way, postcode cb17gx. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TX9jZgqYvjM",
        "outputId": "ba13725e-50aa-4865-8e16-5ae8994bd7c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Is there a hospital nearby?\n",
            "Person 2: Addensbrookes Hospital is located at Hills Rd, Cambridge, postcode CB20QQ. \n",
            "Person 1: Thank you. That's all I need. \n",
            "Person 2: Thank you for using our services.Goodbye.\n",
            "Person 1: Thank you very much. \n",
            "Person 2: You're welcome. Goodbye.\n",
            "Person 1: Goodbye. \n",
            "Person 2: Thank you for using our services.Goodbye.\n",
            "Person 2: Goodby\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6LiQ00iYvjP",
        "outputId": "c241aeaa-93c4-4fa2-83e5-af7d5d86973e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Can you book a taxi?\n",
            "Person 2: I sure can. When would you like to leave? \n",
            "Person 1: I need to leave after 13:00. \n",
            "Person 2: I'm sorry, I have no listings for that time. Would you like to try a different time? \n",
            "Person 1: Yes, let's try 13:00. \n",
            "Person 2: I have booked you a blue audi, contact number: 0775060897490. \n",
            "Person 2: Thank you, that's all of the day sir \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjNpROsQtUlE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}