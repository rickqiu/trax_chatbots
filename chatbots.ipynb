{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC4-4"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent",
        "format_version": "1.3",
        "jupytext_version": "1.5.2"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Chatbots.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickqiu/trax_chatbots/blob/main/chatbots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxKFPeuh_XEm"
      },
      "source": [
        "# Chatbots with Pretrained Reformer Model\n",
        "\n",
        "The notebook demonstrates conversations between two chatbots in single or multiple domains of attraction, hospital, hotel, police, taxi and train using a pretrained Reformer model.\n",
        "\n",
        "Credits should be given to Lukasz Kaiser and his Trax open source team. Here is a list of useful resources.\n",
        "\n",
        "- [MultiWoz](https://arxiv.org/abs/1810.00278) dataset\n",
        "- [Reformer](https://arxiv.org/abs/2001.04451) paper\n",
        "- [Trax](https://github.com/google/trax) github repository\n",
        "- [Natural Language Processing Specialization](https://www.deeplearning.ai/natural-language-processing-specialization/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_S5N0jFY0VQ",
        "outputId": "61cc84f5-c941-429f-e3b4-4de604334508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "!pip install -U https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl\n",
        "!pip install -q -U jax\n",
        "!pip install -q -U trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jaxlib==0.1.55\n",
            "\u001b[?25l  Downloading https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl (144.8MB)\n",
            "\u001b[K     |████████████████████████████████| 144.8MB 27kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.55) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.55) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.55) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jaxlib==0.1.55) (1.15.0)\n",
            "Installing collected packages: jaxlib\n",
            "  Found existing installation: jaxlib 0.1.55\n",
            "    Uninstalling jaxlib-0.1.55:\n",
            "      Successfully uninstalled jaxlib-0.1.55\n",
            "Successfully installed jaxlib-0.1.55\n",
            "\u001b[K     |████████████████████████████████| 481kB 2.8MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 419kB 2.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 13.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 17.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 46.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 49.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6MB 50.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 8.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 41.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 44.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 49.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 655kB 53.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 46.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 51.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 45.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 50.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 49.3MB/s \n",
            "\u001b[?25h  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV4zpTnSVFIp",
        "outputId": "2dab02ed-27e0-4cc0-df6e-cd29d7bb97f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import packages\n",
        "import numpy as np\n",
        "import trax   \n",
        "from trax import layers as tl\n",
        "!pip list | grep trax"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.5                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0kcMWhE4Qw3",
        "outputId": "1cece6ff-2ad3-4c8f-c0ee-6847ab1ac72d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# clone for vocabs and the pretrained model parts from repository\n",
        "!git clone https://github.com/rickqiu/trax_chatbots.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'trax_chatbots'...\n",
            "remote: Enumerating objects: 17, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 34 (delta 8), reused 0 (delta 0), pack-reused 17\u001b[K\n",
            "Unpacking objects: 100% (34/34), done.\n",
            "Checking out files: 100% (11/11), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRO41Ip4467",
        "outputId": "4321a2bc-0a93-4f3e-a631-febf15489da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "%cd trax_chatbots\n",
        "!tar -xzvf vocabs.tar.gz\n",
        "!cat model_splits/* > chatbot_model1.pkl.gz\n",
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/trax_chatbots\n",
            "vocabs/\n",
            "vocabs/en_32k.sentencepiece\n",
            "vocabs/en_32k.sentencepiece.vocab\n",
            "vocabs/en_32k.subword\n",
            "vocabs/en_8k.subword\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkd8zeumYviy"
      },
      "source": [
        "# define attention for fast inference\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "# define the model\n",
        "model = trax.models.reformer.ReformerLM( \n",
        "        # set vocab size\n",
        "        vocab_size=33000,\n",
        "        # set number of layers\n",
        "        n_layers=6,\n",
        "        # set mode\n",
        "        mode='predict',\n",
        "        # set attention type\n",
        "        attention_type=attention\n",
        "    )\n",
        "\n",
        "# define an input signature\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "\n",
        "# initialize from file\n",
        "model.init_from_file('trax_chatbots/chatbot_model1.pkl.gz',\n",
        "                     weights_only=True, input_signature=shape11)\n",
        "\n",
        "# save the starting state\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ7EF6exYvhG"
      },
      "source": [
        "# Reference: Course 4 of https://www.deeplearning.ai/natural-language-processing-specialization/\n",
        "# vocabulary file directory\n",
        "VOCAB_DIR = './trax_chatbots/vocabs'\n",
        "\n",
        "# vocabulary filename\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "\n",
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "\n",
        "\n",
        "def generate_output(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you just trained\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create input tokens using the the tokenize function\n",
        "    input_tokens =  tokenize(start_sentence, vocab_file, vocab_dir)\n",
        "    \n",
        "    # Add batch dimension to array. Convert from (n,) to (x, n) where \n",
        "    # x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)\n",
        "    input_tokens_with_batch = np.expand_dims(input_tokens, axis=0)\n",
        "    \n",
        "    # call the autoregressive_sample_stream function from trax\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
        "        # model\n",
        "        ReformerLM,\n",
        "        # inputs will be the tokens with batch dimension\n",
        "        inputs=input_tokens_with_batch,\n",
        "        # temperature\n",
        "        temperature=temperature\n",
        "    )\n",
        "    \n",
        "    return output_gen\n",
        "\n",
        "\n",
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you just trained\n",
        "        model_state (np.array): initial state of the model before decoding\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        max_len (int): maximum number of tokens to generate \n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"  \n",
        "    \n",
        "    # define the delimiters we used during training\n",
        "    delimiter_1 = 'Person 1: ' \n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    # initialize detokenized output\n",
        "    sentence = ''\n",
        "    \n",
        "    # token counter\n",
        "    counter = 0\n",
        "    \n",
        "    # output tokens. we insert a ': ' for formatting\n",
        "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "    \n",
        "    # reset the model state when starting a new dialogue\n",
        "    ReformerLM.state = model_state\n",
        "    \n",
        "    # calls the output generator implemented earlier\n",
        "    output = generate_output(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    \n",
        "    # print the starting sentence\n",
        "    print(start_sentence.split(delimiter_2)[0].strip())\n",
        "    \n",
        "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\n",
        "    for o in output:\n",
        "        \n",
        "        result.append(o)\n",
        "        \n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        \n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "        \n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        counter += 1\n",
        "        \n",
        "        if counter > max_len:\n",
        "            break    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flhDzydFYvjK",
        "outputId": "7e3b440a-cd4e-4767-a802-3dc873474cc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Are there theatres in town?\n",
            "Person 2: : There are 4 theatres in town. Do you have a preference on area? \n",
            "Person 1: No, I don't care. Which one do you recommend? \n",
            "Person 2: I recommend the Mumford Theatre. Would you like more information on it? \n",
            "Person 1: Yes, I would also like to find a train to Cambridge. \n",
            "Person 2: I have found several trains that meet your criteria. What day would you like to travel? \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TX9jZgqYvjM",
        "outputId": "126700de-5a14-4ec9-bbd4-18b27f215af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Is there a hospital nearby?\n",
            "Person 2: : Addensbrookes Hospital is located at Hills Rd, Cambridge, postcode CB20QQ. Do you need the phone number? \n",
            "Person 1: No, that's all I need. Thank you. \n",
            "Person 2: Thank you for using our services.Q. Have a good day.Good bye.\n",
            "Person 1: Thank you. Goodbye. \n",
            "Person 2: Thank you for using our services.Goodbye.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6LiQ00iYvjP",
        "outputId": "d2217db8-3daa-4c0c-fdfc-31d596498b82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Can you book a taxi?\n",
            "Person 2: : I sure can. When would you like to leave? \n",
            "Person 1: I need to leave after 13:00. \n",
            "Person 2: I'm sorry, I have no listings for that request. Would you like to try a different time? \n",
            "Person 1: No, I need to leave after 13:15. \n",
            "Person 2: I have booked you a grey volkswagen, contact number: 07871762372. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNUnsHDFgBae"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}