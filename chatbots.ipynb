{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC4-4"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent",
        "format_version": "1.3",
        "jupytext_version": "1.5.2"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Chatbots.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickqiu/trax_chatbots/blob/main/chatbots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxKFPeuh_XEm"
      },
      "source": [
        "# Chatbots with Pretrained Reformer Model\n",
        "\n",
        "The notebook demonstrates conversations between two chatbots in single or multiple domains of attraction, hospital, hotel, police, taxi and train using a pretrained Reformer model.\n",
        "\n",
        "Credits should be given to Lukasz Kaiser and his Trax open source team. Here is a list of useful resources.\n",
        "\n",
        "- [MultiWoz](https://arxiv.org/abs/1810.00278) dataset\n",
        "- [Reformer](https://arxiv.org/abs/2001.04451) paper\n",
        "- [Trax](https://github.com/google/trax) github repository\n",
        "- [Natural Language Processing Specialization](https://www.deeplearning.ai/natural-language-processing-specialization/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_S5N0jFY0VQ",
        "outputId": "858b2b53-2157-4de0-c44f-8767e5160a3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "!pip install -U https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl\n",
        "!pip install -q -U jax\n",
        "!pip install -q -U trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting jaxlib==0.1.55\n",
            "\u001b[?25l  Downloading https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl (144.8MB)\n",
            "\u001b[K     |████████████████████████████████| 144.8MB 27kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.12 in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.55) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.55) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from jaxlib==0.1.55) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from absl-py->jaxlib==0.1.55) (1.15.0)\n",
            "Installing collected packages: jaxlib\n",
            "  Found existing installation: jaxlib 0.1.55\n",
            "    Uninstalling jaxlib-0.1.55:\n",
            "      Successfully uninstalled jaxlib-0.1.55\n",
            "Successfully installed jaxlib-0.1.55\n",
            "\u001b[K     |████████████████████████████████| 481kB 2.8MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 419kB 2.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 11.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 28.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 24.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 19.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 41.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 35.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6MB 58.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 65.6MB/s \n",
            "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /packages/bc/fb/f719f1ac965e2101aa6ea6f54ef8b40f8fbb033f6ad07c017663467f5147/pypng-0.0.20.tar.gz\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 655kB 2.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 13.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 13.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 9.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 22.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 20.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 26.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 36.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 50.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.8MB/s \n",
            "\u001b[?25h  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV4zpTnSVFIp",
        "outputId": "2077b58a-6a7a-45d9-b3f4-91886dae0e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# import packages\n",
        "import numpy as np\n",
        "import trax   \n",
        "from trax import layers as tl\n",
        "!pip list | grep trax"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.5                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0kcMWhE4Qw3",
        "outputId": "6351d70b-0f5a-4fd8-b116-a224b3467d20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# clone for vocabs and the pretrained model parts from repository\n",
        "!git clone https://github.com/rickqiu/trax_chatbots.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'trax_chatbots'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 37 (delta 10), reused 0 (delta 0), pack-reused 17\u001b[K\n",
            "Unpacking objects: 100% (37/37), done.\n",
            "Checking out files: 100% (11/11), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRO41Ip4467",
        "outputId": "6917460e-b7e4-454f-8bf6-b742f0617b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "%cd trax_chatbots\n",
        "!tar -xzvf vocabs.tar.gz\n",
        "!cat model_splits/* > chatbot_model1.pkl.gz\n",
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/trax_chatbots\n",
            "vocabs/\n",
            "vocabs/en_32k.sentencepiece\n",
            "vocabs/en_32k.sentencepiece.vocab\n",
            "vocabs/en_32k.subword\n",
            "vocabs/en_8k.subword\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkd8zeumYviy"
      },
      "source": [
        "# define attention for fast inference\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "# define the model\n",
        "model = trax.models.reformer.ReformerLM( \n",
        "        # set vocab size\n",
        "        vocab_size=33000,\n",
        "        # set number of layers\n",
        "        n_layers=6,\n",
        "        # set mode\n",
        "        mode='predict',\n",
        "        # set attention type\n",
        "        attention_type=attention\n",
        "    )\n",
        "\n",
        "# define an input signature\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "\n",
        "# initialize from file\n",
        "model.init_from_file('trax_chatbots/chatbot_model1.pkl.gz',\n",
        "                     weights_only=True, input_signature=shape11)\n",
        "\n",
        "# save the starting state\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ7EF6exYvhG"
      },
      "source": [
        "# Reference: Course 4 of https://www.deeplearning.ai/natural-language-processing-specialization/\n",
        "# vocabulary file directory\n",
        "VOCAB_DIR = './trax_chatbots/vocabs'\n",
        "\n",
        "# vocabulary filename\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "\n",
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "\n",
        "\n",
        "def generate_output(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you just trained\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"\n",
        "    \n",
        "    # Create input tokens using the the tokenize function\n",
        "    input_tokens =  tokenize(start_sentence, vocab_file, vocab_dir)\n",
        "    \n",
        "    # Add batch dimension to array. Convert from (n,) to (x, n) where \n",
        "    # x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)\n",
        "    input_tokens_with_batch = np.expand_dims(input_tokens, axis=0)\n",
        "    \n",
        "    # call the autoregressive_sample_stream function from trax\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
        "        # model\n",
        "        ReformerLM,\n",
        "        # inputs will be the tokens with batch dimension\n",
        "        inputs=input_tokens_with_batch,\n",
        "        # temperature\n",
        "        temperature=temperature\n",
        "    )\n",
        "    \n",
        "    return output_gen\n",
        "\n",
        "\n",
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you just trained\n",
        "        model_state (np.array): initial state of the model before decoding\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        max_len (int): maximum number of tokens to generate \n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"  \n",
        "    \n",
        "    # define the delimiters we used during training\n",
        "    delimiter_1 = 'Person 1: ' \n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    # initialize detokenized output\n",
        "    sentence = ''\n",
        "    \n",
        "    # token counter\n",
        "    counter = 0\n",
        "    \n",
        "    # output tokens. we insert a ': ' for formatting\n",
        "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "    \n",
        "    # reset the model state when starting a new dialogue\n",
        "    ReformerLM.state = model_state\n",
        "    \n",
        "    # calls the output generator implemented earlier\n",
        "    output = generate_output(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    \n",
        "    # print the starting sentence\n",
        "    print(start_sentence.split(delimiter_2)[0].strip())\n",
        "    \n",
        "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\n",
        "    for o in output:\n",
        "        \n",
        "        result.append(o)\n",
        "        \n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        \n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "        \n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        counter += 1\n",
        "        \n",
        "        if counter > max_len:\n",
        "            break    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flhDzydFYvjK",
        "outputId": "45de6445-ce25-4dfa-b0ab-a10f7b2c268a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Are there theatres in town?\n",
            "Person 2: : There are 4 theatres in town. Do you have a preference? \n",
            "Person 1: Not really, but I would like the most affordable and the cambridge corn exchange. \n",
            "Person 2: I would recommend the Mumford Theatre. Would you like more information on it? \n",
            "Person 1: Yes, could I get the postcode and phone number please? \n",
            "Person 2: The phone number is 08451962320 and the postcode is cb11pt. \n",
            "Person 2: Thank bybybybyby\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TX9jZgqYvjM",
        "outputId": "9eefc8a8-00a8-4229-d075-ea6e712399ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Is there a hospital nearby?\n",
            "Person 2: : Addensbrookes Hospital is located at Hills Rd, Cambridge, Postcode CB20QQ. Do you need a particular department? \n",
            "Person 1: No, but I do need the phone number, please. \n",
            "Person 2: The phone number is 01223245151. Do you need the main phone number? \n",
            "Person 1: No, that's all I need. Thank you! \n",
            "Person 2: You're welcome. Have a nice day.\n",
            "Person 1: Thank you find me one called the gandhi. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6LiQ00iYvjP",
        "outputId": "f11d8510-2a9b-41ab-ee0c-0f2cbae5af7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Can you book a taxi?\n",
            "Person 2: : I sure can. Where would you like to be picked up? \n",
            "Person 1: I'm going to be from the hamilton lodge restaurant. \n",
            "Person 2: What time would you like to be picked up? \n",
            "Person 1: I need to be picked up by 23:45. \n",
            "Person 2: Booking completed!\n",
            "Booked car type\t:\tgrey volkswagen\n",
            "Contact number\t:\t07180084574\n",
            " \n",
            "Person 2: Thank bybybybybyby\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}