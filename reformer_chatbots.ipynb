{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC4-4"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent",
        "format_version": "1.3",
        "jupytext_version": "1.5.2"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Chatbots.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickqiu/trax_chatbots/blob/main/reformer_chatbots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz0ZPlIoEZug"
      },
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\")\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "\n",
        " https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxKFPeuh_XEm"
      },
      "source": [
        "# Chatbot with Reformer Model\n",
        "@author: Rick Qiu\n",
        "\n",
        "This notebook demonstrates a chatbot with a Reformer model pre-trained on the MultiWoz dataset. The chatbot is built with Google Trax, which is a low-code and high-speed deep learning library.\n",
        "\n",
        "Although the chatbot can not compete with Alexa, Siri, Cortana and Meena, it gives reasonable good conversations in the domains of attraction, hotel, taxi, train, hospital and police.\n",
        "\n",
        "Through this notebook, not only one will learn how to create an AI chatbot with few lines of code, but one will also have the opportunities of seeing the algorithm i.e., the model structure and chatting to the AI.\n",
        "\n",
        "### References\n",
        "\n",
        "- [MultiWoz](https://arxiv.org/abs/1810.00278) dataset\n",
        "- [Reformer](https://arxiv.org/abs/2001.04451) paper\n",
        "- [Trax](https://github.com/google/trax) code repository\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocUMh33iJTN7"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_S5N0jFY0VQ",
        "outputId": "4049e265-0428-409e-8067-40397f29d882",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!pip install -q -U https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl\n",
        "!pip install -q -U jax\n",
        "!pip install -q -U trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 144.8MB 27kB/s \n",
            "\u001b[K     |████████████████████████████████| 491kB 6.5MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 419kB 4.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 15.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 44.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 46.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3MB 48.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6MB 58.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 68.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 655kB 63.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 60.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 53.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 11.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 61.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 60.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 62.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 66.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 56.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 68.7MB/s \n",
            "\u001b[?25h  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zok0zyauGzh8",
        "outputId": "278d0bf0-6341-4fd0-9722-82cfdb22805e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip list | grep trax"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.5                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0kcMWhE4Qw3",
        "outputId": "a56b34ff-523f-459b-ffa3-a24678ccae21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# clone to get vocabs and the pretrained model parts from repository\n",
        "!git clone https://github.com/rickqiu/trax_chatbots.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'trax_chatbots'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Counting objects: 100% (72/72), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 89 (delta 43), reused 0 (delta 0), pack-reused 17\u001b[K\n",
            "Unpacking objects: 100% (89/89), done.\n",
            "Checking out files: 100% (11/11), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRO41Ip4467",
        "outputId": "45838374-0061-40fe-ef7c-c3865e0377c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "%cd trax_chatbots\n",
        "!tar -xzvf vocabs.tar.gz\n",
        "!cat model_splits/* > chatbot_model1.pkl.gz\n",
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/trax_chatbots\n",
            "vocabs/\n",
            "vocabs/en_32k.sentencepiece\n",
            "vocabs/en_32k.sentencepiece.vocab\n",
            "vocabs/en_32k.subword\n",
            "vocabs/en_8k.subword\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV4zpTnSVFIp"
      },
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import trax   \n",
        "from trax import layers as tl"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJP-0qKjJlVu"
      },
      "source": [
        "## 2. Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkd8zeumYviy"
      },
      "source": [
        "# define attention for fast inference\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 240\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "# define the model\n",
        "model = trax.models.reformer.ReformerLM( \n",
        "        vocab_size=33000,\n",
        "        n_layers=6,\n",
        "        mode='predict', # default 'train' for model training\n",
        "        attention_type=attention\n",
        "    )"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtGMPmIWIaSo",
        "outputId": "0c11975f-29a6-4dc1-cf0f-8594068c9822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# display the Reformer model\n",
        "print(str(model))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  ShiftRight(1)\n",
            "  Embedding_33000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Dup_out2\n",
            "  ReversibleSerial_in2_out2[\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "  ]\n",
            "  Concatenate_in2\n",
            "  LayerNorm\n",
            "  Dropout\n",
            "  Dense_33000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjQBBmg8J0xB"
      },
      "source": [
        "## 3. Using pre-traind model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58Q35cz7JAW5"
      },
      "source": [
        "# define an input signature\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "\n",
        "# initialize the model from file\n",
        "model.init_from_file('trax_chatbots/chatbot_model1.pkl.gz',\n",
        "                     weights_only=True, input_signature=shape11)\n",
        "\n",
        "# save the starting state\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKWpcVvjulm4"
      },
      "source": [
        "## 4. Code for making prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ7EF6exYvhG"
      },
      "source": [
        "# https://www.deeplearning.ai/natural-language-processing-specialization/\n",
        "# vocabulary file directory\n",
        "VOCAB_DIR = './trax_chatbots/vocabs'\n",
        "\n",
        "# vocabulary filename\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "\n",
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "\n",
        "\n",
        "def generate_output(model, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model:  the Reformer language model you just trained\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"\n",
        "    \n",
        "    input_tokens =  tokenize(start_sentence, vocab_file, vocab_dir)\n",
        "    \n",
        "    # add batch dimension to array\n",
        "    input_tokens_with_batch = np.expand_dims(input_tokens, axis=0)\n",
        "    \n",
        "    # call the autoregressive_sample_stream function from trax\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
        "        model,\n",
        "        inputs=input_tokens_with_batch,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    \n",
        "    return output_gen\n",
        "\n",
        "\n",
        "def generate_sentence(model, model_state, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model:  the Reformer language model you just trained\n",
        "        model_state (np.array): initial state of the model before decoding\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"  \n",
        "    \n",
        "    # define the delimiters we used during training\n",
        "    delimiter_1 = 'Person 1: ' \n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    # initialize detokenized output\n",
        "    sentence = ''\n",
        "\n",
        "    # output tokens\n",
        "    result = []\n",
        "    \n",
        "    # reset the model state when starting a new dialogue\n",
        "    model.state = model_state\n",
        "    \n",
        "    # calls the output generator implemented earlier\n",
        "    output = generate_output(model, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    \n",
        "    # print the starting sentence\n",
        "    #print(start_sentence.split(delimiter_2)[0].strip())\n",
        "    \n",
        "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\n",
        "    for o in output:\n",
        "        \n",
        "        result.append(o)\n",
        "        \n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        \n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            break\n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            break"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXYcPuBFKBNO"
      },
      "source": [
        "## 5. Making a conversatoin with AI\n",
        "\n",
        "Run the following cell to start a conversation input. Enter 'q' to exit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TX9jZgqYvjM",
        "outputId": "64cf8174-c066-4bf4-f7d9-6c9c6f5e7327",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "sample_sentence = ''\n",
        "in_msg = ''\n",
        "\n",
        "while True:\n",
        "  in_msg = input('Person 1: ')\n",
        "  if in_msg == 'q':\n",
        "    break\n",
        "    \n",
        "  sample_sentence += ' Person 1: ' + in_msg + ' Person 2: '\n",
        "  generate_sentence(model, model_state=STARTING_STATE, \n",
        "  start_sentence=sample_sentence,\n",
        "  vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, \n",
        "  temperature=0.2)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Are there theatres in town?\n",
            "Person 2: There are 4 theatres in town. Do you have a preference on area? \n",
            "Person 1: No, I don't care. Which one would you recommend?\n",
            "Person 2: I would recommend the Mumford Theatre. It's located at Anglia Ruskin Enterprise, east road. \n",
            "Person 1: Yes, could I get the postcode and phone number?\n",
            "Person 2: The phone number is 01223332360 and the postcode is cb58as. \n",
            "Person 1: Thank you very much.\n",
            "Person 2: You're welcome. The postcode is cb58as. \n",
            "Person 1: q\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}