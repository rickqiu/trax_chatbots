{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC4-4"
      ]
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent",
        "format_version": "1.3",
        "jupytext_version": "1.5.2"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Chatbots.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rickqiu/trax_chatbots/blob/main/chatbots_conversation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxKFPeuh_XEm"
      },
      "source": [
        "# AI Chatbots Conversation\n",
        "@author: Rick Qiu\n",
        "\n",
        "This notebook demonstrates examples of conversations between two chatbots, who have local knowledge in multiple domains such as attraction, hospital, hotel, police, taxi and train. \n",
        "\n",
        "The demo uses Google Trax deep learning library and a pre-trained Reformer language model \"chatbot_model1.pkl.gz\" from COURSERA NLP Specialization. \n",
        "\n",
        "Here is a list of useful resources.\n",
        "\n",
        "\n",
        "- [MultiWoz](https://arxiv.org/abs/1810.00278) dataset\n",
        "- [Reformer](https://arxiv.org/abs/2001.04451) paper\n",
        "- [Trax](https://github.com/google/trax) code repository\n",
        "- COURSERA [NLP Specialization](https://www.coursera.org/specializations/natural-language-processing)\n",
        "\n",
        "### Acknowledgements\n",
        "Many thanks to Łukasz Kaiser and his Google Trax open source project team members\n",
        "\n",
        "Special thanks to NLP Specialization instructor Younes Bensouda Mourri and all staff\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocUMh33iJTN7"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_S5N0jFY0VQ",
        "outputId": "f45366f0-619b-4c4b-fcb8-cbdc1734c219",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        }
      },
      "source": [
        "!pip install -q -U https://storage.googleapis.com/jax-releases/cuda101/jaxlib-0.1.55-cp36-none-manylinux2010_x86_64.whl\n",
        "!pip install -q -U jax\n",
        "!pip install -q -U trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 144.8MB 27kB/s \n",
            "\u001b[K     |████████████████████████████████| 481kB 6.5MB/s \n",
            "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 419kB 8.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 21.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 57.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 174kB 42.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 62.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 348kB 56.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 655kB 68.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 194kB 63.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 368kB 71.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 58.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 983kB 63.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 10.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 72.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.6MB 72.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 8.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1MB 52.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 63.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 63.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 65.5MB/s \n",
            "\u001b[?25h  Building wheel for pypng (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for bz2file (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: kfac 0.2.3 has requirement tensorflow-probability==0.8, but you'll have tensorflow-probability 0.7.0 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zok0zyauGzh8",
        "outputId": "817ea181-1200-4611-a34c-b0ae0c37fcfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "!pip list | grep trax"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trax                          1.3.5                \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0kcMWhE4Qw3",
        "outputId": "3ff49f4d-5876-4a3e-c31d-9f494fe95d41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "# clone to get vocabs and the pretrained model parts from repository\n",
        "!git clone https://github.com/rickqiu/trax_chatbots.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'trax_chatbots'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 72 (delta 32), reused 0 (delta 0), pack-reused 17\u001b[K\n",
            "Unpacking objects: 100% (72/72), done.\n",
            "Checking out files: 100% (11/11), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrRO41Ip4467",
        "outputId": "e19c40b1-5f8c-4f42-b464-3fb37e11e3b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "%cd trax_chatbots\n",
        "!tar -xzvf vocabs.tar.gz\n",
        "!cat model_splits/* > chatbot_model1.pkl.gz\n",
        "%cd .."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/trax_chatbots\n",
            "vocabs/\n",
            "vocabs/en_32k.sentencepiece\n",
            "vocabs/en_32k.sentencepiece.vocab\n",
            "vocabs/en_32k.subword\n",
            "vocabs/en_8k.subword\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aV4zpTnSVFIp"
      },
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import trax   \n",
        "from trax import layers as tl"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJP-0qKjJlVu"
      },
      "source": [
        "## 2. Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkd8zeumYviy"
      },
      "source": [
        "# define attention for fast inference\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 512\n",
        "    kwargs['predict_drop_len'] = 128\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "# define the model\n",
        "model = trax.models.reformer.ReformerLM( \n",
        "        vocab_size=33000,\n",
        "        n_layers=6,\n",
        "        mode='predict',\n",
        "        attention_type=attention\n",
        "    )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtGMPmIWIaSo",
        "outputId": "cf67659b-064f-463c-d9e0-f8f69c3b5519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# display the Reformer model\n",
        "print(str(model))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serial[\n",
            "  ShiftRight(1)\n",
            "  Embedding_33000_512\n",
            "  Dropout\n",
            "  PositionalEncoding\n",
            "  Dup_out2\n",
            "  ReversibleSerial_in2_out2[\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "      ]\n",
            "      SelfAttention\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "    ReversibleHalfResidual_in2_out2[\n",
            "      Serial[\n",
            "        LayerNorm\n",
            "        Dense_2048\n",
            "        Dropout\n",
            "        FastGelu\n",
            "        Dense_512\n",
            "        Dropout\n",
            "      ]\n",
            "    ]\n",
            "    ReversibleSwap_in2_out2\n",
            "  ]\n",
            "  Concatenate_in2\n",
            "  LayerNorm\n",
            "  Dropout\n",
            "  Dense_33000\n",
            "  LogSoftmax\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjQBBmg8J0xB"
      },
      "source": [
        "## 3.  Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58Q35cz7JAW5"
      },
      "source": [
        "# define an input signature\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "\n",
        "# initialize the model from file\n",
        "model.init_from_file('trax_chatbots/chatbot_model1.pkl.gz',\n",
        "                     weights_only=True, input_signature=shape11)\n",
        "\n",
        "# save the starting state\n",
        "STARTING_STATE = model.state"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ7EF6exYvhG"
      },
      "source": [
        "# https://www.deeplearning.ai/natural-language-processing-specialization/\n",
        "# vocabulary file directory\n",
        "VOCAB_DIR = './trax_chatbots/vocabs'\n",
        "\n",
        "# vocabulary filename\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "\n",
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "\n",
        "\n",
        "def generate_output(model, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model:  the Reformer language model you just trained\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"\n",
        "    \n",
        "    input_tokens =  tokenize(start_sentence, vocab_file, vocab_dir)\n",
        "    \n",
        "    # add batch dimension to array\n",
        "    input_tokens_with_batch = np.expand_dims(input_tokens, axis=0)\n",
        "    \n",
        "    # call the autoregressive_sample_stream function from trax\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
        "        model,\n",
        "        inputs=input_tokens_with_batch,\n",
        "        temperature=temperature\n",
        "    )\n",
        "    \n",
        "    return output_gen\n",
        "\n",
        "\n",
        "def generate_dialogue(model, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        model:  the Reformer language model you just trained\n",
        "        model_state (np.array): initial state of the model before decoding\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        max_len (int): maximum number of tokens to generate \n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"  \n",
        "    \n",
        "    # define the delimiters we used during training\n",
        "    delimiter_1 = 'Person 1: ' \n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    # initialize detokenized output\n",
        "    sentence = ''\n",
        "    \n",
        "    # token counter\n",
        "    counter = 0\n",
        "\n",
        "    # turns\n",
        "    turns = 0\n",
        "    \n",
        "    # output tokens\n",
        "    result = []\n",
        "    \n",
        "    # reset the model state when starting a new dialogue\n",
        "    model.state = model_state\n",
        "    \n",
        "    # calls the output generator implemented earlier\n",
        "    output = generate_output(model, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    \n",
        "    # print the starting sentence\n",
        "    print(start_sentence.split(delimiter_2)[0].strip())\n",
        "\n",
        "    # turns\n",
        "    turns = 1\n",
        "    \n",
        "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\n",
        "    for o in output:\n",
        "        \n",
        "        result.append(o)\n",
        "        \n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        \n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "            turns += 1\n",
        "        \n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "            turns +=1\n",
        "\n",
        "        counter += 1\n",
        "        \n",
        "        if counter > max_len and turns%2 == 0:\n",
        "           break    "
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXYcPuBFKBNO"
      },
      "source": [
        "## 4. Outputing results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flhDzydFYvjK",
        "outputId": "b1aa82ca-d729-4861-e6e3-9e75acc7fb08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
        "generate_dialogue(model, model_state=STARTING_STATE, \n",
        "                  start_sentence=sample_sentence, \n",
        "                  vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, \n",
        "                  max_len=120, temperature=0.2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Are there theatres in town?\n",
            "Person 2: There are 4 theatres in town. Two are in the centre of town and 1 in the south. Do you have a preference? \n",
            "Person 1: No, but I would like the one in the south. \n",
            "Person 2: There are 4 theatres in the south. The south, south, and south. Which would you prefer? \n",
            "Person 1: I would prefer the south. \n",
            "Person 2: I would recommend the nusha. Would you like more information? \n",
            "Person 1: Yes, could I get the postcode and phone number? \n",
            "Person 2: The phone number is 01223902112 and the postcode is cb17dy. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TX9jZgqYvjM",
        "outputId": "b388c8af-9a9e-42d9-84c2-ae535f8ff8db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
        "generate_dialogue(model, model_state=STARTING_STATE, \n",
        "                  start_sentence=sample_sentence,\n",
        "                  vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, \n",
        "                  max_len=120, temperature=0.2)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Is there a hospital nearby?\n",
            "Person 2: Addensbrookes Hospital is located at Hills Rd, Cambridge, postcode CB20QQ. Do you need anything else? \n",
            "Person 1: No, that's all. Thank you. \n",
            "Person 2: You're welcome. Have a nice day.\n",
            "Person 1: Thank you for your help. \n",
            "Person 2: You're welcome. Have a nice day.\n",
            "Person 1: Thanks again. Goodbye. \n",
            "Person 2: Thank you for using our services.Goodbye.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6LiQ00iYvjP",
        "outputId": "5b0fc680-1359-4272-84c9-8bbf5a901a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
        "generate_dialogue(model, model_state=STARTING_STATE, \n",
        "                  start_sentence=sample_sentence, \n",
        "                  vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, \n",
        "                  max_len=120, temperature=0.2)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Person 1: Can you book a taxi?\n",
            "Person 2: I sure can. Where would you like to be picked up? \n",
            "Person 1: I'm going to be picked up from the Museum of Classical Archaeology.  \n",
            "Person 2: I'd be happy to help. What time would you like to be picked up? \n",
            "Person 1: I need to be picked up by 13:00. \n",
            "Person 2: Booking completed! Booked car type\t:\tgrey ford\n",
            "Contact number\t:\t07180084574\n",
            " \n",
            "Person 1: Thank you. That's all I need. \n",
            "Person 2: You're welcome. Have a good day!Bye.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}